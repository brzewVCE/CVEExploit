import requests
from bs4 import BeautifulSoup
import requests
import re
from termcolor import colored
from time import sleep
from pyxploitdb import searchCVE
import string
import pandas as pd

class ShodanScraper:
    def __init__(self, proxies, verbose=False):
        self.verbose = verbose
        self.proxies = proxies


    def veryfiy_page(self, response):
        if response.status_code != 200:
            print(colored("Error: Response code: {}".format(response.status_code), "red"))
            return False
        elif "Daily search usage limit reached" in response.text:
            print(colored("Daily search usage limit reached", "yellow"))
            return "Somewhat working"
        elif "Please create a" in response.text:
            print(colored("Please create an account to view more results.", "yellow"))
            return False
        elif "Result limit reached." in response.text:
            print(colored("Result limit reached.", "yellow"))
            return False
        else:
            return True
    
    def index_to_string(self,index):
        chars = string.ascii_letters + string.digits  # A-Z, a-z, 0-9
        base = len(chars)  # Base is the number of characters we are cycling through

        # Calculate string length based on index
        length = 1
        while index >= base ** length:
            index -= base ** length
            length += 1

        # Generate the string for the given index
        result = ""
        for _ in range(length):
            index, char_index = divmod(index, base)
            result = chars[char_index] + result

        return result

    def search_shodan(self, process_index="+"):
        self.process_index = process_index
        self.search_running = True
        #check for .csv file existence
        filename =  "found_ips.csv"
        try:
            with open(filename, "r") as file:
                #if file exists, read it
                pass
        except FileNotFoundError:
            self.search_index = 0
            #create a .csv file with headers
            data = {
                "ip": [], 
                "query": [], 
                "index": [], 
                "current_index": []  }
            df = pd.DataFrame(data)
            if self.verbose:
                print(df)
            df.to_csv(filename, index=False)
            df = pd.read_csv(filename)
            df["current_index"] = [self.search_index]
            df.to_csv(filename, index=False)

        df = pd.read_csv(filename)
        self.search_index = int(df["current_index"][0])
        #loop through pages
        while self.search_running:
            try:
                df = pd.read_csv(filename)
                self.search_index = df["current_index"][0]
                current_index = self.search_index
                if self.verbose:
                    print(colored(f"Current search index: {self.search_index}", "green"))
                    print(df)
                    dft = df.to_dict()
                    print(dft)
                # Generate the string for the given index
                query = self.index_to_string(self.search_index)

                print(f"[ {process_index} ]"+colored(f"Searching Shodan for: {query}", "blue"))
                response, proxy = self.proxies.url_request(f"https://www.shodan.io/search?query={query}")
                print(response)
                if not response:
                    print(colored(f"Error with {proxy} switching proxy", "red"))
                    continue
                soup = BeautifulSoup(response.text, 'html.parser')
                if self.veryfiy_page(response) != True:
                    if self.verbose:
                        print(colored(f"Error with {proxy} switching proxy", "red"))
                    continue
                if "No results found" in response.text:
                    if self.verbose:
                        print(colored(f"No IP addresses found for query: {query}", "red"))
        
                        df["current_index"][0] = self.search_index + 1
                        df.to_csv(filename, index=False)
                    continue
                
                # Print the page source if verbose
                if self.verbose:
                    print(soup.prettify())

                found_ips = []
                for div in soup.find_all("div", class_="heading"):
                    a = div.find("a", class_="title text-dark")
                    if a:
                        ip = a['href'][6:]
                        found_ips.append(ip) 
                if len(found_ips) == 0:
                    if self.verbose:
                        print(colored(f"No IP addresses found for query: {query}", "red"))
                    if current_index >= self.search_index:
                        df["current_index"][0] = current_index
                        df.to_csv(filename, index=False)
                    continue

                else:
                    if self.verbose:
                        print(colored("IP addresses found for query: {}".format(query), "green"))
                        print(colored(found_ips, "green"))
                    
                    for ip in found_ips:
                        if ip not in df["ip"].values and re.match(r"\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$", ip):
                            new_row = {'ip': ip, 'query': query, 'index': current_index,}
                            new_row = pd.DataFrame(new_row, index=[0])
                            with open(filename, 'a', newline='') as f:
                                new_row.to_csv(f, header=False, index=False)
                            if self.verbose:
                                print(colored(f"New IP address added to the file: {ip}", "green"))
                        else:
                            if self.verbose:
                                print(colored(f"IP address already exists in the file: {ip}", "yellow"))
                
                    if current_index >= self.page_index:
                        df["current_index"][0] = current_index
                        df.to_csv(filename, index=False)
                    sleep(5)
                    self.proxies.confirm_working_proxy(proxy)
                sleep(1)
                continue
            except KeyboardInterrupt:
                self.running = False
                return print(colored("Exiting", "red"))
            except Exception as e:
                if self.verbose:
                    return print(e)
                continue


    def get_exploits(self):
        filename = str(self.query) + "_ips.txt"
        #check for file existence
        try:
            with open(filename, "r") as file:
                pass
        except FileNotFoundError:
            return print("File not found")

        with open(filename, "r") as file:
            headers = {
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'
                    }
            for ip in file:
                ip = ip.strip()
                print(ip)
                url = str("https://internetdb.shodan.io/"+ip)
                print(url)
                print("Searching CVE's for: {}".format(ip))
                try:
                    # Get response from Shodan API
                    response = requests.get(url).json()
                    #print(response)
                    cve_list = response["vulns"]

                    # Print scraped CVE's
                    print("CVE's found for IP address {}:".format(ip))
                    if len(cve_list) > 0:
                        print(colored(cve_list, "green"))
                    else:
                        print(colored(f"No CVE's found for IP address: {ip} :(", "red"))
                    # Search for exploits in ExploitDB
                    print(colored("Searching ExploitDB for exploits...", "light_magenta"))
                    for cve in cve_list:
                        result = searchCVE(cve)
                        if result != []:
                            print(f"{cve} found in ExploitDB")
                            print(colored(result, "green"))
                            for exploit in result:
                                print(exploit.link)
                except Exception as e:
                    return print(e)
                