import requests
from bs4 import BeautifulSoup
import requests
import re
from termcolor import colored

class ShodanScraper:
    def __init__(self, query, proxies):
        self.query = query
        self.proxies = proxies


    def veryfiy_page(self, response):
        if response.status_code != 200:
            print(colored("Error: Response code: {}".format(response.status_code), "red"))
            return False
        elif "Daily search usage limit reached" in response.text:
            print(colored("Daily search usage limit reached", "yellow"))
            return "Somewhat working"
        elif "Please create an account to view more results." in response.text:
            print(colored("Please create an account to view more results.", "yellow"))
            return False
        elif "Result limit reached." in response.text:
            print(colored("Result limit reached.", "yellow"))
            return False
    

    def search_shodan(self, index=1):
        self.index = index
        self.ip_addresses = []
        #loop through pages
        try:
            while True:
                query = self.query + '&page=' + str(self.index)
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'
                    }
                url = 'http://www.shodan.io/search?query={}'.format(query)
                print("Searching Shodan for: {}".format(query))

                try:
                    proxy = self.proxies.get_proxies()
                    response = requests.get(url, proxies=proxy,headers=headers, timeout=5)
                except Exception as e:
                    print(colored("Error - switching proxy", "red"))
                    continue

                soup = BeautifulSoup(response.text, 'html.parser')
                proxy = str(proxy)[17:-2]

                if self.veryfiy_page(response) == False:
                    print("Error, switching proxy")
                    continue
                elif self.veryfiy_page(response) == "Somewhat working":
                    print(colored(f"Proxy somewhat working: {proxy}", "blue"))
                    with open("working_proxies.txt", "a") as file:
                        pass
                    with open("working_proxies.txt", "a") as file:
                        file.write(str(proxy) + "\n")
                    continue
                elif self.veryfiy_page(response) == True:
                    print(colored(f"Proxy working: {proxy}", "green"))
                    with open("working_proxies.txt", "a") as file:
                        pass
                    with open("working_proxies.txt", "a") as file:
                        file.write(str(proxy) + "\n")
                    print(colored(soup.prettify(), "green"))

                
                print(colored("Page verified", "green"))
                self.index += 1

                for div in soup.find_all("div", class_="heading"):
                    a = div.find("a", class_="title text-dark")
                    if a:
                        self.ip_addresses.append(a['href'][6:])
                
                if len(self.ip_addresses) == 0:
                    print("No IP addresses found for query: {}".format(query))

                if self.ip_addresses != []:
                    print(self.ip_addresses)
        except KeyboardInterrupt:
            return print(colored("Exiting", "red"))



    def list_ips(self):
        for ip in self.search_shodan() or []:
            print(ip)
            filename = str(self.query) + "_ips.txt"
            # if filename doesnt exist, create it
            with open(filename, "a") as file:
                pass
            # Code to execute if IP is in proper format and not in file
            if re.match(r"\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$", ip) and ip not in open(filename).read():
                with open(filename, "a") as file:
                    file.write(ip + "\n")

    def get_CVEs(self):
        filename = str(self.query) + "_ips.txt"
        #check for file existence
        try:
            with open(filename, "r") as file:
                pass
        except FileNotFoundError:
            return print("File not found")

        with open(filename, "r") as file:
            for ip in file:
                try:
                    print(ip)
                    url = 'https://www.shodan.io/host/{}'.format(ip)
                    print("Searching CVE's for: {}".format(ip))
                    response = requests.get(url)
                    print("Response code: {}".format(response.status_code))
                    soup = BeautifulSoup(response.text, 'html.parser')
                    print(soup.prettify())
                    if self.veryfiy_page(response) == False:
                        return 0
                    else:
                        print("Page verified")
                    cve_table = soup.find('table', id='vulnerabilities')
                    if cve_table:
                        print("Scraping CVE's for IP address: {}".format(ip))
                        
                        cve_list = []
                        for row in cve_table.find_all('tr'):
                            a = row.find("a", class_="title text-dark")
                            if a:
                                cve_list.append(a['href'][19:])
                                
                        # Print scraped CVE's
                        print("CVE's found for IP address {}:".format(ip))
                        print("\n".join(cve_list))
        
                    else:
                        print("No CVE's found for IP address: {}".format(ip))
                        
                    
                except Exception as e:
                    return print(e)
                