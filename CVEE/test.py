import requests
from bs4 import BeautifulSoup
import requests
import re
from termcolor import colored
from time import sleep
from pyxploitdb import searchCVE
import string
import pandas as pd

class ShodanScraper:
    def __init__(self, proxies, verbose=False):
        self.verbose = verbose
        self.proxies = proxies


    def veryfiy_page(self, response):
        if response.status_code != 200:
            print(colored("Error: Response code: {}".format(response.status_code), "red"))
            return False
        elif "Daily search usage limit reached" in response.text:
            print(colored("Daily search usage limit reached", "yellow"))
            return "Somewhat working"
        elif "Please create a" in response.text:
            print(colored("Please create an account to view more results.", "yellow"))
            return False
        elif "Result limit reached." in response.text:
            print(colored("Result limit reached.", "yellow"))
            return False
        else:
            return True
    
    def index_to_string(self,index):
        chars = string.ascii_letters + string.digits  # A-Z, a-z, 0-9
        base = len(chars)  # Base is the number of characters we are cycling through

        # Calculate string length based on index
        length = 1
        while index >= base ** length:
            index -= base ** length
            length += 1

        # Generate the string for the given index
        result = ""
        for _ in range(length):
            index, char_index = divmod(index, base)
            result = chars[char_index] + result

        return result

    def search_shodan(self, process_index="+"):
        self.process_index = process_index
        self.search_running = True
        #check for .csv file existence
        filename =  "found_ips.csv"
        try:
            with open(filename, "r") as file:
                #if file exists, read it
                pass
        except FileNotFoundError:
            self.search_index = 0
            #create a .csv file with headers
            data = {
                "ip": [], 
                "query": [], 
                "index": [], 
                "current_index": []  }
            df = pd.DataFrame(data)
            if self.verbose:
                print(df)
            df.to_csv(filename, index=False)
            df = pd.read_csv(filename)
            df["current_index"] = [self.search_index]
            df.to_csv(filename, index=False)

        df = pd.read_csv(filename)
        print(df)
        self.search_index = int(df["current_index"].iloc[0])
        if self.verbose:
            print(f"Current index: {self.search_index}")
        #loop through pages
        while self.search_running:
            df = pd.read_csv(filename)
            # Generate the string for the given index
            current_index = self.search_index
            query = self.index_to_string(current_index)

            print(f"[ {process_index} ]"+colored(f"Searching Shodan for: {query}", "blue"))
            response, proxy = self.proxies.url_request(f"https://www.shodan.io/search?query={query}")
            if response == None:
                print(colored(f"Error with {proxy} switching proxy", "red"))
                continue
            soup = BeautifulSoup(response.text, 'html.parser')
            if self.veryfiy_page(response) != True:
                if self.verbose:
                    print(colored(f"Error with {proxy} switching proxy", "red"))
                continue
            if "No results found" in response.text:
                if self.verbose:
                    print(colored(f"No IP addresses found for query: {query}", "red"))
                if current_index >= self.search_index:
                    df["current_index"] = current_index
                    df.to_csv(filename, index=False)
                continue
            
            # Print the page source if verbose
            if self.verbose:
                print(soup.prettify())

            found_ips = []
            for div in soup.find_all("div", class_="heading"):
                a = div.find("a", class_="title text-dark")
                if a:
                    ip = a['href'][6:]
                    found_ips.append(ip) 
            if len(found_ips) == 0:
                if self.verbose:
                    print(colored(f"No IP addresses found for query: {query}", "red"))
                if current_index >= self.search_index:
                    df["current_index"] = current_index
                    df.to_csv(filename, index=False)
                continue

            else:
                if self.verbose:
                    print(colored("IP addresses found for query: {}".format(query), "green"))
                    print(colored(found_ips, "green"))
                
                for ip in found_ips:
                    if ip not in df["ip"].values and re.match(r"\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$", ip):
                        new_row = {'ip': ip, 'query': query, 'index': current_index,}
                        with open(filename, 'a') as f:
                            new_row.to_csv(f, header=False, index=False)
                        if self.verbose:
                            print(colored(f"New IP address added to the file: {ip}", "green"))
                    else:
                        if self.verbose:
                            print(colored(f"IP address already exists in the file: {ip}", "yellow"))
            
                if current_index >= self.page_index:
                    self.page_index += 1
                sleep(5)
                self.proxies.confirm_working_proxy(proxy)
            sleep(1)
            continue