import requests
from bs4 import BeautifulSoup
import requests
import re
from termcolor import colored


class Proxy:
    def __init__(self, proxy_source="working_proxies.txt", verbose=False):
        self.proxy_source = proxy_source
        self.verbose = verbose
        if proxy_source == "default_http_proxies.txt":
            self.get_proxy_list()
    
    def url_request(self, url, timeout=10):
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.101 Safari/537.36'}
            proxy = self.get_proxy()
            response = requests.get(url, proxies=proxy,headers=headers, timeout=timeout)
            return response,proxy
        except Exception as e:
            if self.verbose:
                print(e)
            return False, proxy

    def confirm_working_proxy(self, proxy):
        try:
            #convert proxy dict to string
            proxy_string = str(proxy["http"][7:])
            print(proxy_string)
            with open("working_proxies.txt", "a") as file:
                pass
            if proxy_string not in open("working_proxies.txt").read():
                with open("working_proxies.txt", "a") as file:
                    file.write(str(proxy_string) + "\n")
            return print(colored(f"Working proxy: {proxy_string}", "green"))
        except Exception as e:
            if self.verbose:
                print(e)
            return None


    def veryfiy_ip(self, ip):
        try:
            if re.match(r"\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:\d{1,5}", ip) == None:
                return False
            else:
                return True
        except Exception as e:
            return False

    def get_proxy_list(self):
        filename = "default_http_proxies.txt"
        try:
            # create file if it doesn't exist
            with open(filename, "a") as file:
                pass
            # get proxies from proxyscrape.com
            url = "https://api.proxyscrape.com/v2/?request=displayproxies&protocol=http&timeout=10000&country=all&ssl=all&anonymity=all&auth="
            response = requests.get(url)
            # save proxies to file
            for ip in response.text.split("\n"):
                if ip not in open(filename).read():
                    with open(filename, "a") as file:
                        file.write(ip)
            # count how many proxies were saved
            ips_found = len(response.text.split("\n"))
            if self.verbose:
                print(colored(f"{ips_found} IP's saved to {filename}", "green"))
            return None
        except Exception as e:
            return print(e)

    def get_proxy(self):
        try:
            filename = str(self.proxy_source)
            # check if there are proxies left in the file, switch to default if not
            with open(filename, "r") as file:
                proxy_list = file.readlines()
                proxy_list = [x.strip() for x in proxy_list]
                print(proxy_list)
            
            
            if proxy_list == []:
                if filename == "default_http_proxies.txt":
                    self.get_proxy_list()
                filename = "default_http_proxies.txt"
            # get proxy and remove it from the file
            with open(filename, "r") as file:
                proxy_list = file.readlines()
            while self.veryfiy_ip(proxy_list[0]) == False:
                del proxy_list[0]
            proxy = proxy_list[0].strip()
            del proxy_list[0]
            with open(filename, "w") as file:
                file.writelines(proxy_list)
            # convert proxy to dict
            proxy = {
                "http": "http://" + str(proxy),
                "https": "http://" + str(proxy),
            }
            if self.verbose:
                print(colored(f"Using proxy: {proxy}", "blue"))
            return proxy
        except Exception as e:
            return print(e)

