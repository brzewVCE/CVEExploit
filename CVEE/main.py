import requests
from bs4 import BeautifulSoup
import requests
import re
import proxyscrape


class ShodanScraper:
    def __init__(self, query, proxies):
        self.query = query
        self.proxies = proxies

    def veryfiy_page(self, response):
        if response.status_code != 200:
            print("Error: Response code: {}".format(response.status_code))
            return False
        elif "Daily search usage limit reached" in response.text:
            print("Daily search usage limit reached")
            return False
        elif "Please create an account to view more results." in response.text:
            print("Please create an account to view more results.")
            return False
        elif "Result limit reached." in response.text:
            print("Result limit reached.")
            return False
        elif "Max retries exceeded" in response.text:
                return False
        else:
            return True

    def search_shodan(self, index=1):
        self.index = index
        self.ip_addresses = []
        try:
            #loop through pages
            while True:
                query = self.query + '&page=' + str(self.index)
                url = 'http://www.shodan.io/search?query={}'.format(query)
                print("Searching Shodan for: {}".format(query))
                response = requests.get(url, proxies=self.proxies.get_proxies("http"))
                #check for response code
                print("Response code: {}".format(response.status_code))
                if self.veryfiy_page(response) == False:
                    print("Proxy error, switching proxy")
                    continue
                soup = BeautifulSoup(response.text, 'html.parser')
                print(soup.prettify())

                #Return ips if page not found, but ip gathered
                if self.veryfiy_page(response) == False and self.ip_addresses != []:
                    return self.ip_addresses
                #Return error if page not found and no ip gathered
                if self.veryfiy_page(response) == False :
                    return print("Error: Page not verified")

                else:
                    print("Page verified")
                    self.index += 1

                for div in soup.find_all("div", class_="heading"):
                    a = div.find("a", class_="title text-dark")
                    if a:
                        self.ip_addresses.append(a['href'][6:])
                
                if len(self.ip_addresses) == 0:
                    return print("No IP addresses found for query: {}".format(query))


        except Exception as e:
            return print(e)

    def list_ips(self):
        for ip in self.search_shodan() or []:
            print(ip)
            filename = str(self.query) + "_ips.txt"
            # if filename doesnt exist, create it
            with open(filename, "a") as file:
                pass
            # Code to execute if IP is in proper format and not in file
            if re.match(r"\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$", ip) and ip not in open(filename).read():
                with open(filename, "a") as file:
                    file.write(ip + "\n")
    
    def get_CVEs(self):
        filename = str(self.query) + "_ips.txt"
        #check for file existence
        try:
            with open(filename, "r") as file:
                pass
        except FileNotFoundError:
            return print("File not found")

        with open(filename, "r") as file:
            for ip in file:
                try:
                    print(ip)
                    url = 'https://www.shodan.io/host/{}'.format(ip)
                    print("Searching CVE's for: {}".format(ip))
                    response = requests.get(url)
                    print("Response code: {}".format(response.status_code))
                    soup = BeautifulSoup(response.text, 'html.parser')
                    print(soup.prettify())
                    if self.veryfiy_page(response) == False:
                        return 0
                    else:
                        print("Page verified")
                    cve_table = soup.find('table', id='vulnerabilities')
                    if cve_table:
                        print("Scraping CVE's for IP address: {}".format(ip))
                        
                        cve_list = []
                        for row in cve_table.find_all('tr'):
                            a = row.find("a", class_="title text-dark")
                            if a:
                                cve_list.append(a['href'][19:])
                                
                        # Print scraped CVE's
                        print("CVE's found for IP address {}:".format(ip))
                        print("\n".join(cve_list))
        
                    else:
                        print("No CVE's found for IP address: {}".format(ip))
                        
                    
                except Exception as e:
                    return print(e)
                
class ExploitSearch:
    def __init__(self, ip):
        self.ip = ip

class Proxy:
    def __init__(self):
        pass

    def get_proxy_list(self, protocol):
        try:
            url = "https://api.proxyscrape.com/v2/?request=displayproxies&protocol=" + protocol + "&timeout=10000&country=all&ssl=all&anonymity=all&auth="
            response = requests.get(url)
            filename = str(protocol)+"_proxies.txt"
            with open(filename, "w") as file:
                file.write(response.text.replace('\n', ''))
            return print(f"{response.text} saved to {filename}")
        except Exception as e:
            return print(e)
        
    def get_proxies(self, protocol):
        try:
            filename = str(protocol) + "_proxies.txt"
            with open(filename, "r") as file:
                proxy_list = file.readlines()
                proxy = proxy_list[0].strip()
                del proxy_list[0]
            with open(filename, "w") as file:
                file.writelines(proxy_list)
            proxy = {protocol: str(protocol)+"://"+str(proxy)}
            print(f"Using proxy: {proxy}")
            return proxy
        except Exception as e:
            return print(e)

if __name__ == "__main__":
    proxy = Proxy()
    # proxy_list =  proxy.get_proxy_list("http")
    # proxy = proxy.get_proxies("http")
    scraper = ShodanScraper("olsztyn",proxy)
    scraper.list_ips()
    #scraper.get_CVEs()
    #switch between proxies if daily limit reached
        
