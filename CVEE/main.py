import requests
from bs4 import BeautifulSoup
import requests
import re
import proxyscrape

class ShodanScraper:
    def __init__(self, query, proxies):
        self.query = query
        self.proxies = proxies

    def veryfiy_page(self, response):
        if response.status_code != 200:
            print("Error: Response code: {}".format(response.status_code))
            return False
        if "Error: Daily search usage limit reached" in response.text:
            print("Daily search usage limit reached")
            return False
        if "Please create an account to view more results." in response.text:
            print("Please create an account to view more results.")
            return False
        if "Daily search usage limit reached. Please create a free account to do more searches." in response.text:
            print("Daily search usage limit reached. Please create a free account to do more searches.")
            return False
        else:
            return True

    def search_shodan(self):
        try:
            #loop through pages
            i = 1
            while True:
                query = self.query + '&page=' + str(i)
                url = 'http://www.shodan.io/search?query={}'.format(query)
                print("Searching Shodan for: {}".format(query))
                response = requests.get(url, proxies=self.proxies)
                print("Response code: {}".format(response.status_code))
                soup = BeautifulSoup(response.text, 'html.parser')
                print(soup.prettify())

                ip_addresses = []
                
                if self.veryfiy_page(response) == False and ip_addresses != []:
                    return ip_addresses

                for div in soup.find_all("div", class_="heading"):
                    a = div.find("a", class_="title text-dark")
                    if a:
                        ip_addresses.append(a['href'][6:])
                
                if len(ip_addresses) == 0:
                    return print("No IP addresses found for query: {}".format(query))
                i += 1

                return ip_addresses
        except Exception as e:
            return print(e)

    def list_ips(self):
        for ip in self.search_shodan() or []:
            print(ip)
            filename = str(self.query) + "_ips.txt"
            # if filename doesnt exist, create it
            with open(filename, "a") as file:
                pass
            # Code to execute if IP is in proper format and not in file
            if re.match(r"\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$", ip) and ip not in open(filename).read():
                with open(filename, "a") as file:
                    file.write(ip + "\n")
    
    def get_CVEs(self):
        filename = str(self.query) + "_ips.txt"
        #check for file existence
        try:
            with open(filename, "r") as file:
                pass
        except FileNotFoundError:
            return print("File not found")

        with open(filename, "r") as file:
            for ip in file:
                try:
                    print(ip)
                    url = 'https://www.shodan.io/host/{}'.format(ip)
                    print("Searching CVE's for: {}".format(ip))
                    response = requests.get(url)
                    print("Response code: {}".format(response.status_code))
                    soup = BeautifulSoup(response.text, 'html.parser')
                    print(soup.prettify())
                    if self.veryfiy_page(response) == False:
                        return 0
                    cve_table = soup.find('table', id='vulnerabilities')
                    if cve_table:
                        print("Scraping CVE's for IP address: {}".format(ip))
                        
                        cve_list = []
                        for row in cve_table.find_all('tr'):
                            a = row.find("a", class_="title text-dark")
                            if a:
                                cve_list.append(a['href'][19:])
                                
                        # Print scraped CVE's
                        print("CVE's found for IP address {}:".format(ip))
                        print("\n".join(cve_list))
        
                    else:
                        print("No CVE's found for IP address: {}".format(ip))
                        
                    
                except Exception as e:
                    return print(e)
                
class ExploitSearch:
    def __init__(self, ip):
        self.ip = ip

class Proxy:
    def __init__(self):
        pass

    def get_proxy_list(self, protocol):
        try:
            url = "https://api.proxyscrape.com/v2/?request=displayproxies&protocol=" + protocol + "&timeout=10000&country=all&ssl=all&anonymity=all&auth="
            response = requests.get(url)
            filename = str(protocol)+"_proxies.txt"
            with open(filename, "w") as file:
                file.write(response.text.replace('\n', ''))
            return print(f"{response.text} saved to {filename}")
        except Exception as e:
            return print(e)
    def get_proxies(self, protocol):
        try:
            filename = str(protocol) + "_proxies.txt"
            with open(filename, "r") as file:
                proxy_list = file.readlines()
                proxy = proxy_list[0].strip()
                del proxy_list[0]
            with open(filename, "w") as file:
                file.writelines(proxy_list)
            proxy = {"http": "http://"+str(proxy)}
            print(f"Using proxy: {proxy}")
            return proxy
        except Exception as e:
            return print(e)
            
        except Exception as e:
            return print(e)
if __name__ == "__main__":
    proxy = Proxy()
    proxy_list =  proxy.get_proxy_list("http")
    proxy = proxy.get_proxies("http")
    scraper = ShodanScraper("olsztyn",proxy)
    while True:
        scraper.list_ips()
    #scraper.get_CVEs()
