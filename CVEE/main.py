import requests
from bs4 import BeautifulSoup
import requests
import re

class ShodanScraper:
    def __init__(self, query, proxies):
        self.query = query
        self.proxies = proxies

    def veryfiy_page(self, response):
        if response.status_code != 200:
            print("Error: Response code: {}".format(response.status_code))
            return False
        if "Error: Daily search usage limit reached" in response.text:
            print("Daily search usage limit reached")
            return False
        if "Please create an account to view more results." in response.text:
            print("Please create an account to view more results.")
            return False
        if "Daily search usage limit reached. Please create a free account to do more searches." in response.text:
            print("Daily search usage limit reached. Please create a free account to do more searches.")
            return False
        else:
            return True

    def search_shodan(self):
        try:
            #loop through pages
            i = 1
            while True:
                query = self.query + '&page=' + str(i)
                url = 'https://www.shodan.io/search?query={}'.format(query)
                print("Searching Shodan for: {}".format(query))
                response = requests.get(url, proxies=self.proxies)
                print("Response code: {}".format(response.status_code))
                soup = BeautifulSoup(response.text, 'html.parser')
                print(soup.prettify())

                ip_addresses = []
                
                if self.veryfiy_page(response) == False:
                    break

                for div in soup.find_all("div", class_="heading"):
                    a = div.find("a", class_="title text-dark")
                    if a:
                        ip_addresses.append(a['href'][6:])
                
                if len(ip_addresses) == 0:
                    return print("No IP addresses found for query: {}".format(query))
                i += 1
            return ip_addresses
        except Exception as e:
            return print(e)

    def list_ips(self):
        for ip in self.search_shodan():
            print(ip)
            filename = str(self.query) + "_ips.txt"
            # if filename doesnt exist, create it
            with open(filename, "a") as file:
                pass
            # Code to execute if IP is in proper format and not in file
            if re.match(r"\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$", ip) and ip not in open(filename).read():
                with open(filename, "a") as file:
                    file.write(ip + "\n")
    
    def get_CVEs(self):
        filename = str(self.query) + "_ips.txt"
        #check for file existence
        try:
            with open(filename, "r") as file:
                pass
        except FileNotFoundError:
            return print("File not found")

        with open(filename, "r") as file:
            for ip in file:
                try:
                    print(ip)
                    url = 'https://www.shodan.io/host/{}'.format(ip)
                    print("Searching CVE's for: {}".format(ip))
                    response = requests.get(url)
                    print("Response code: {}".format(response.status_code))
                    soup = BeautifulSoup(response.text, 'html.parser')
                    print(soup.prettify())
                    if self.veryfiy_page(response) == False:
                        return 0
                    cve_table = soup.find('table', id='vulnerabilities')
                    if cve_table:
                        print("Scraping CVE's for IP address: {}".format(ip))
                        
                        cve_list = []
                        for row in cve_table.find_all('tr'):
                            a = row.find("a", class_="title text-dark")
                            if a:
                                cve_list.append(a['href'][19:])
                                
                        # Print scraped CVE's
                        print("CVE's found for IP address {}:".format(ip))
                        print("\n".join(cve_list))
        
                    else:
                        print("No CVE's found for IP address: {}".format(ip))
                        
                    
                except Exception as e:
                    return print(e)
                
class ExploitSearch:
    def __init__(self, ip):
        self.ip = ip

class GetProxy:
    def __init__(self):
        self.proxies = {"http": [], "https": []}
    def proxies(self):
        try:
            with open("proxies.txt", "r") as file:
                for line in file:
                    protocol, proxy = line.strip().split("://")
                    if protocol in self.proxies:
                        self.proxies[protocol].append(proxy)
            return self.proxies
        except Exception as e:
            print(e)
if __name__ == "__main__":
    proxies = GetProxy().proxies
    scraper = ShodanScraper("olsztyn",proxies)
    scraper.list_ips()
    #scraper.get_CVEs()
